{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1d8470-5aa5-4ffa-a6a5-6a6cd34d03bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 12:35:02.254933: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-02 12:35:03.366130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "100%|██████████| 1024/1024 [00:38<00:00, 26.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 1/5\n",
      "Epoch 1/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4805 - loss: 0.6927 - val_accuracy: 0.5000 - val_loss: 0.6987\n",
      "Epoch 2/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.5035 - loss: 0.6834 - val_accuracy: 0.5061 - val_loss: 0.7123\n",
      "Epoch 3/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.4829 - loss: 0.6805 - val_accuracy: 0.5061 - val_loss: 0.7173\n",
      "Epoch 4/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.5168 - loss: 0.6651 - val_accuracy: 0.5061 - val_loss: 0.7340\n",
      "Fold 1 Accuracy: 0.5000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.99      0.66       165\n",
      "           1       0.50      0.01      0.02       165\n",
      "\n",
      "    accuracy                           0.50       330\n",
      "   macro avg       0.50      0.50      0.34       330\n",
      "weighted avg       0.50      0.50      0.34       330\n",
      "\n",
      "Confusion Matrix:\n",
      " [[163   2]\n",
      " [163   2]]\n",
      "Training Fold 2/5\n",
      "Epoch 1/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.4921 - loss: 0.6940 - val_accuracy: 0.5167 - val_loss: 0.6900\n",
      "Epoch 2/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5124 - loss: 0.6855 - val_accuracy: 0.5167 - val_loss: 0.6920\n",
      "Epoch 3/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.5177 - loss: 0.6790 - val_accuracy: 0.5137 - val_loss: 0.6994\n",
      "Epoch 4/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4918 - loss: 0.6739 - val_accuracy: 0.5106 - val_loss: 0.7024\n",
      "Fold 2 Accuracy: 0.5167\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.07      0.12       165\n",
      "           1       0.51      0.97      0.67       164\n",
      "\n",
      "    accuracy                           0.52       329\n",
      "   macro avg       0.60      0.52      0.39       329\n",
      "weighted avg       0.60      0.52      0.39       329\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 11 154]\n",
      " [  5 159]]\n",
      "Training Fold 3/5\n",
      "Epoch 1/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.4755 - loss: 0.6940 - val_accuracy: 0.5228 - val_loss: 0.6901\n",
      "Epoch 2/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4933 - loss: 0.6864 - val_accuracy: 0.5106 - val_loss: 0.7018\n",
      "Epoch 3/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5200 - loss: 0.6791 - val_accuracy: 0.5137 - val_loss: 0.7167\n",
      "Epoch 4/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.5008 - loss: 0.6689 - val_accuracy: 0.5106 - val_loss: 0.7559\n",
      "Fold 3 Accuracy: 0.5228\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.07      0.13       165\n",
      "           1       0.51      0.98      0.67       164\n",
      "\n",
      "    accuracy                           0.52       329\n",
      "   macro avg       0.63      0.52      0.40       329\n",
      "weighted avg       0.63      0.52      0.40       329\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 12 153]\n",
      " [  4 160]]\n",
      "Training Fold 4/5\n",
      "Epoch 1/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.5133 - loss: 0.6936 - val_accuracy: 0.4894 - val_loss: 0.6944\n",
      "Epoch 2/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5113 - loss: 0.6858 - val_accuracy: 0.5258 - val_loss: 0.7152\n",
      "Epoch 3/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5042 - loss: 0.6808 - val_accuracy: 0.5228 - val_loss: 0.7120\n",
      "Epoch 4/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.4916 - loss: 0.6718 - val_accuracy: 0.4894 - val_loss: 0.7321\n",
      "Fold 4 Accuracy: 0.4894\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.97      0.65       164\n",
      "           1       0.29      0.01      0.02       165\n",
      "\n",
      "    accuracy                           0.49       329\n",
      "   macro avg       0.39      0.49      0.34       329\n",
      "weighted avg       0.39      0.49      0.34       329\n",
      "\n",
      "Confusion Matrix:\n",
      " [[159   5]\n",
      " [163   2]]\n",
      "Training Fold 5/5\n",
      "Epoch 1/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5089 - loss: 0.6940 - val_accuracy: 0.5046 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5240 - loss: 0.6840 - val_accuracy: 0.5137 - val_loss: 0.6933\n",
      "Epoch 3/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5034 - loss: 0.6780 - val_accuracy: 0.5076 - val_loss: 0.6964\n",
      "Epoch 4/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.5110 - loss: 0.6744 - val_accuracy: 0.5076 - val_loss: 0.6978\n",
      "Fold 5 Accuracy: 0.5046\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.99      0.67       164\n",
      "           1       0.67      0.02      0.05       165\n",
      "\n",
      "    accuracy                           0.50       329\n",
      "   macro avg       0.58      0.51      0.36       329\n",
      "weighted avg       0.58      0.50      0.36       329\n",
      "\n",
      "Confusion Matrix:\n",
      " [[162   2]\n",
      " [161   4]]\n",
      "Average Accuracy across 5 folds: 0.5067\n",
      "Epoch 1/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.4804 - loss: 0.6921\n",
      "Epoch 2/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.4998 - loss: 0.6860\n",
      "Epoch 3/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5304 - loss: 0.6771\n",
      "Epoch 4/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.5292 - loss: 0.6709\n",
      "Epoch 5/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5273 - loss: 0.6699\n",
      "Epoch 6/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5339 - loss: 0.6662\n",
      "Epoch 7/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.4985 - loss: 0.6566\n",
      "Epoch 8/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.5330 - loss: 0.6630\n",
      "Epoch 9/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5136 - loss: 0.6655\n",
      "Epoch 10/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5188 - loss: 0.6582\n",
      "Epoch 11/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5613 - loss: 0.6593\n",
      "Epoch 12/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5429 - loss: 0.6597\n",
      "Epoch 13/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5241 - loss: 0.6562\n",
      "Epoch 14/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5409 - loss: 0.6601\n",
      "Epoch 15/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5305 - loss: 0.6603\n",
      "Epoch 16/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.5178 - loss: 0.6629\n",
      "Epoch 17/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.5377 - loss: 0.6595\n",
      "Epoch 18/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5365 - loss: 0.6593\n",
      "Epoch 19/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5201 - loss: 0.6531\n",
      "Epoch 20/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5153 - loss: 0.6607\n",
      "Final Model Accuracy: 0.5334\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE  # Import SMOTE for oversampling\n",
    "\n",
    "# Define paths\n",
    "annotation_dir = \"./InAra-Corpus/plagiarism-annotation\"\n",
    "text_dir = \"./InAra-Corpus/suspicious-documents\"\n",
    "glove_path = \"glove.6B.100d.txt\"\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embeddings_index = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char if char.isalnum() or char.isspace() else ' ' for char in text])  # Remove special characters\n",
    "    text = ' '.join(text.split())  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Create a list to store data\n",
    "data = []\n",
    "\n",
    "# Iterate through annotation files\n",
    "for filename in tqdm(os.listdir(annotation_dir)):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        filepath = os.path.join(annotation_dir, filename)\n",
    "        tree = ET.parse(filepath)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Extract document reference\n",
    "        doc_id = root.attrib['reference']\n",
    "        \n",
    "        # Extract plagiarism information (if present)\n",
    "        plagiarism_segments = []\n",
    "        for feature in root.findall('feature'):\n",
    "            if feature.attrib['name'] == 'plagiarism':\n",
    "                start = int(feature.attrib['this_offset'])\n",
    "                end = start + int(feature.attrib['this_length'])\n",
    "                plagiarism_segments.append((start, end))\n",
    "        \n",
    "        # Read the corresponding text file\n",
    "        text_path = os.path.join(text_dir, doc_id)\n",
    "        with open(text_path, 'r', encoding='cp1256') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        # Preprocess the text\n",
    "        preprocessed_text = preprocess_text(text)\n",
    "        \n",
    "        # Create a data entry\n",
    "        data.append({\n",
    "            'text': preprocessed_text,\n",
    "            'plagiarism_segments': plagiarism_segments,\n",
    "            'doc_id': doc_id\n",
    "        })\n",
    "\n",
    "# Define parameters\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 100\n",
    "NUM_FOLDS = 5\n",
    "DROPOUT_RATE = 0.2\n",
    "EPOCHS = 20\n",
    "PATIENCE = 3\n",
    "\n",
    "# Create tokenizer and convert text to sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([d['text'] for d in data])\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences([d['text'] for d in data])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# Create embedding matrix\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < num_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Create labels\n",
    "labels = np.array([1 if d['plagiarism_segments'] else 0 for d in data])\n",
    "\n",
    "# Apply SMOTE to oversample the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(padded_sequences, labels)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(X_resampled, y_resampled), 1):\n",
    "    print(f\"Training Fold {fold}/{NUM_FOLDS}\")\n",
    "    X_train, X_test = X_resampled[train_idx], X_resampled[test_idx]\n",
    "    y_train, y_test = y_resampled[train_idx], y_resampled[test_idx]\n",
    "    \n",
    "    # Build CNN Model\n",
    "    model = Sequential([\n",
    "        Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False),\n",
    "        Conv1D(128, 3, activation='relu'),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        Conv1D(64, 3, activation='relu'),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    fold_accuracies.append(accuracy)\n",
    "    print(f\"Fold {fold} Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Additional metrics\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Average accuracy across folds\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"Average Accuracy across {NUM_FOLDS} folds: {average_accuracy:.4f}\")\n",
    "\n",
    "# Train the final model on all data\n",
    "final_model = Sequential([\n",
    "    Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False),\n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    Conv1D(64, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "final_model.fit(X_resampled, y_resampled, epochs=EPOCHS, batch_size=32, verbose=1)\n",
    "\n",
    "# Final evaluation\n",
    "final_loss, final_accuracy = final_model.evaluate(X_resampled, y_resampled, verbose=0)\n",
    "print(f\"Final Model Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56972041-6432-4238-961e-9963d51e5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save(\"model.keras\")\n",
    "\n",
    "import pickle\n",
    "with open(\"tokenizer.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
